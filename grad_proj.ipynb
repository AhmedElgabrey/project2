{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Elgabrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Elgabrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Elgabrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Elgabrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Elgabrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"A parameter name that contains\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn import metrics\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import textstat\n",
    "import joblib\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i just feel really helpless and heavy hearted</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ive enjoyed being able to slouch about relax a...</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i gave up my internship with the dmrg and am f...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i dont know i feel so lost</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am a kindergarten teacher and i am thoroughl...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422741</th>\n",
       "      <td>i begun to feel distressed for you</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422742</th>\n",
       "      <td>i left feeling annoyed and angry thinking that...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422743</th>\n",
       "      <td>i were to ever get married i d have everything...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422744</th>\n",
       "      <td>i feel reluctant in applying there because i w...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422745</th>\n",
       "      <td>i just wanted to apologize to you because i fe...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>422746 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "0           i just feel really helpless and heavy hearted   fear\n",
       "1       ive enjoyed being able to slouch about relax a...    sad\n",
       "2       i gave up my internship with the dmrg and am f...   fear\n",
       "3                              i dont know i feel so lost    sad\n",
       "4       i am a kindergarten teacher and i am thoroughl...   fear\n",
       "...                                                   ...    ...\n",
       "422741                 i begun to feel distressed for you   fear\n",
       "422742  i left feeling annoyed and angry thinking that...  anger\n",
       "422743  i were to ever get married i d have everything...    joy\n",
       "422744  i feel reluctant in applying there because i w...   fear\n",
       "422745  i just wanted to apologize to you because i fe...  anger\n",
       "\n",
       "[422746 rows x 2 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_and_prepare_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load dataset and rename columns.\n",
    "\n",
    "    Args:\n",
    "        path (str): File path to CSV dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Prepared dataframe with columns ['text', 'label'].\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(path)\n",
    "    data.rename(columns={'sentence': 'text', 'emotion': 'label'}, inplace=True)\n",
    "    return data\n",
    "\n",
    "data = load_and_prepare_data(r\"D:\\Grad_Proj\\project\\combined_emotion.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 422746 entries, 0 to 422745\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    422746 non-null  object\n",
      " 1   label   422746 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 6.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No_of_Chars</th>\n",
       "      <th>No_of_Words</th>\n",
       "      <th>No_of_Sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>422746.000000</td>\n",
       "      <td>422746.000000</td>\n",
       "      <td>422746.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>97.033980</td>\n",
       "      <td>19.220179</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>56.198156</td>\n",
       "      <td>11.057121</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>86.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>128.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>830.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         No_of_Chars    No_of_Words  No_of_Sents\n",
       "count  422746.000000  422746.000000     422746.0\n",
       "mean       97.033980      19.220179          1.0\n",
       "std        56.198156      11.057121          0.0\n",
       "min         2.000000       1.000000          1.0\n",
       "25%        54.000000      11.000000          1.0\n",
       "50%        86.000000      17.000000          1.0\n",
       "75%       128.000000      25.000000          1.0\n",
       "max       830.000000     178.000000          1.0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['No_of_Chars'] = data['text'].apply(len)\n",
    "data['No_of_Words'] = data.apply(lambda row: nltk.word_tokenize(row['text']), axis= 1).apply(len)\n",
    "data['No_of_Sents'] = data.apply(lambda row: nltk.sent_tokenize(row['text']), axis= 1).apply(len)\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(df: pd.DataFrame, col: str) -> (pd.DataFrame, LabelEncoder):\n",
    "    \"\"\"\n",
    "    Encode categorical labels to integers.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        col (str): Column name of labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Dataframe with encoded labels, fitted LabelEncoder.\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    return df, le\n",
    "\n",
    "data, le = encode_labels(data, 'label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" cols_color = ['black', 'blue', 'red', 'green', 'purple', 'cyan']\\nplt.figure(figsize=(12,8))\\nfg = sns.countplot(x= data['label'], palette= cols_color)\\nfg.set_title('count plot of classes')\\nfg.set_xlabel('classes')\\nfg.set_ylabel('count of classes') \""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" cols_color = ['black', 'blue', 'red', 'green', 'purple', 'cyan']\n",
    "plt.figure(figsize=(12,8))\n",
    "fg = sns.countplot(x= data['label'], palette= cols_color)\n",
    "fg.set_title('count plot of classes')\n",
    "fg.set_xlabel('classes')\n",
    "fg.set_ylabel('count of classes') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" plt.figure(figsize=(12,8))\\nfg = sns.pairplot(data= data, hue= 'label', palette= cols_color)\\nplt.show(fg) \""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" plt.figure(figsize=(12,8))\n",
    "fg = sns.pairplot(data= data, hue= 'label', palette= cols_color)\n",
    "plt.show(fg) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"\n",
    "    تحويل POS tag من شكل NLTK إلى الشكل المطلوب من WordNetLemmatizer.\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # الافتراضي\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    تنظيف النص مع تحسين lemmatization باستخدام POS tagging.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmatized_words = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
    "        for word, tag in pos_tags\n",
    "        if word not in stop_words\n",
    "    ]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "data['lemmatized_words'] = data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" cols_color = ['black', 'blue', 'red', 'green', 'purple', 'cyan']\\nplt.figure(figsize=(12,8))\\nfg = sns.countplot(x= data['label'], palette= cols_color)\\nfg.set_title('count plot of classes')\\nfg.set_xlabel('classes')\\nfg.set_ylabel('count of classes') \""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" cols_color = ['black', 'blue', 'red', 'green', 'purple', 'cyan']\n",
    "plt.figure(figsize=(12,8))\n",
    "fg = sns.countplot(x= data['label'], palette= cols_color)\n",
    "fg.set_title('count plot of classes')\n",
    "fg.set_xlabel('classes')\n",
    "fg.set_ylabel('count of classes') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' all_words = \" \".join(sentence for sentence in data[\\'lemmatized_words\\'])\\nall_words\\n\\nwordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)\\n\\nplt.figure(figsize=(12,8))\\nplt.imshow(wordcloud, interpolation=\\'bilinear\\')\\nplt.axis(\\'off\\')\\nplt.show() '"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" all_words = \" \".join(sentence for sentence in data['lemmatized_words'])\n",
    "all_words\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=1000, max_df=0.9, min_df=2, stop_words='english', ngram_range=(1, 2))\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vectorize_text(df: pd.DataFrame, text_col: str, vectorizer: TfidfVectorizer):\n",
    "    \"\"\"\n",
    "    Vectorize text column using TF-IDF.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe containing text data.\n",
    "        text_col (str): Name of the column with preprocessed text.\n",
    "        vectorizer (TfidfVectorizer): Initialized vectorizer.\n",
    "\n",
    "    Returns:\n",
    "        sparse matrix: TF-IDF features matrix.\n",
    "    \"\"\"\n",
    "    X = vectorizer.fit_transform(df[text_col])\n",
    "    return X\n",
    "\n",
    "sampled_data = data.sample(frac=0.3, random_state=42)\n",
    "X = vectorize_text(sampled_data, 'lemmatized_words', tfidf)\n",
    "y = sampled_data['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_additional_features(text: str) -> list:\n",
    "    words = word_tokenize(text)\n",
    "    num_words = len(words)\n",
    "    num_chars = len(text)\n",
    "    sentiment = sia.polarity_scores(text)['compound']\n",
    "    readability = textstat.flesch_reading_ease(text)\n",
    "    return [num_words, num_chars, sentiment, readability]\n",
    "\n",
    "additional_features = np.array([extract_additional_features(text) for text in sampled_data['lemmatized_words']])\n",
    "scaler = StandardScaler()\n",
    "additional_features_scaled = scaler.fit_transform(additional_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "X_combined = hstack([X, csr_matrix(additional_features_scaled)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52840                      NN IN NN JJ NN VB JJ JJ NN NN NN\n",
       "338919    VB JJ NN RB VBP RB JJ NN VBP RB JJ NN VBP JJ R...\n",
       "4335                                              VBG NN NN\n",
       "252892                        VB IN NN JJ NN NN VB JJ NN RB\n",
       "334554               VB RB JJ JJ JJ NN NN JJ NN JJ NN NN NN\n",
       "                                ...                        \n",
       "31865                               NN IN JJ NN NN VB IN NN\n",
       "72273                                           NN NN NN NN\n",
       "138646                             JJ NN RB JJ RB VBP NN NN\n",
       "158786                       NN NN RB JJ NN VB NN NN VBP NN\n",
       "202847                                          NN RB JJ NN\n",
       "Name: pos_tags, Length: 126824, dtype: object"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_pos_tags(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    return ' '.join(tag for word, tag in pos_tags)\n",
    "\n",
    "sampled_data['pos_tags'] = sampled_data['lemmatized_words'].apply(extract_pos_tags)\n",
    "\n",
    "tfidf_pos = TfidfVectorizer()\n",
    "X_pos = tfidf_pos.fit_transform(sampled_data['pos_tags'])\n",
    "sampled_data['pos_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: [14272 11980 34326  8252 28936  3693]\n",
      "After SMOTE: [34326 34326 34326 34326 34326 34326]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Before SMOTE:\", np.bincount(y_train))\n",
    "print(\"After SMOTE:\", np.bincount(y_train_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: Counter({2: 34326, 4: 28936, 0: 14272, 1: 11980, 3: 8252, 5: 3693})\n",
      "After SMOTE: Counter({2: 34326, 1: 34326, 0: 34326, 4: 34326, 5: 34326, 3: 34326})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"Before SMOTE:\", Counter(y_train))\n",
    "print(\"After SMOTE:\", Counter(y_train_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 80: 0.5824834849061667\n",
      "index 134: 0.5461345627470632\n",
      "index 261: 0.08891625239402497\n",
      "index 327: 0.25158305605751424\n",
      "index 453: 0.4915822572203815\n",
      "index 579: 0.22270334424009006\n"
     ]
    }
   ],
   "source": [
    "row = X[0].toarray().flatten()\n",
    "for i, val in enumerate(row):\n",
    "    if val != 0:\n",
    "        print(f'index {i}: {val}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7797\n",
      "Test set F1 Score (weighted): 0.7802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.75      0.79      3568\n",
      "           1       0.78      0.71      0.74      2995\n",
      "           2       0.82      0.82      0.82      8582\n",
      "           3       0.66      0.70      0.68      2063\n",
      "           4       0.78      0.81      0.80      7234\n",
      "           5       0.56      0.65      0.60       923\n",
      "\n",
      "    accuracy                           0.78     25365\n",
      "   macro avg       0.74      0.74      0.74     25365\n",
      "weighted avg       0.78      0.78      0.78     25365\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.83      0.75      0.79      3568\n",
      "        fear       0.78      0.71      0.74      2995\n",
      "         joy       0.82      0.82      0.82      8582\n",
      "        love       0.66      0.70      0.68      2063\n",
      "         sad       0.78      0.81      0.80      7234\n",
      "     suprise       0.56      0.65      0.60       923\n",
      "\n",
      "    accuracy                           0.78     25365\n",
      "   macro avg       0.74      0.74      0.74     25365\n",
      "weighted avg       0.78      0.78      0.78     25365\n",
      "\n",
      "F1 Score: 0.7802174019905194\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train Random Forest and evaluate on test set.\n",
    "\n",
    "    Args:\n",
    "        X_train, y_train: Training features and labels.\n",
    "        X_test, y_test: Testing features and labels.\n",
    "\n",
    "    Returns:\n",
    "        model: Trained Random Forest model.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    \"\"\"  param_grid = {\n",
    "    'n_estimators': [100, 200, 300],           # عدد الأشجار\n",
    "    'max_depth': [None, 10, 20, 30],           # أقصى عمق للشجرة\n",
    "    'min_samples_split': [2, 5, 10]             # أقل عدد عينات لتقسيم عقدة\n",
    "    } \"\"\"\n",
    "    \"\"\"  \n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid,cv=3, n_jobs=-1, verbose=2, scoring='f1_weighted')\n",
    "    grid_search.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    print(\"أفضل المعاملات:\", grid_search.best_params_)\n",
    "    print(\"أفضل نتيجة F1 (weighted):\", grid_search.best_score_)\n",
    "    \n",
    "    best_rf_model = grid_search.best_estimator_ \"\"\"\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Test set F1 Score (weighted): {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    \"\"\" cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show() \"\"\"\n",
    "    \n",
    "    print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "    return model\n",
    "\n",
    "random_forest_model = train_and_evaluate(X_train_balanced, y_train_balanced, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "نفس الموديل بس بطريقة اسرع لأنه يجرب عينات عشوائية من المعاملات بدل كل التوليفات:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7797\n",
      "Test set F1 Score (weighted): 0.7802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.75      0.79      3568\n",
      "           1       0.78      0.71      0.74      2995\n",
      "           2       0.82      0.82      0.82      8582\n",
      "           3       0.66      0.70      0.68      2063\n",
      "           4       0.78      0.81      0.80      7234\n",
      "           5       0.56      0.65      0.60       923\n",
      "\n",
      "    accuracy                           0.78     25365\n",
      "   macro avg       0.74      0.74      0.74     25365\n",
      "weighted avg       0.78      0.78      0.78     25365\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.83      0.75      0.79      3568\n",
      "        fear       0.78      0.71      0.74      2995\n",
      "         joy       0.82      0.82      0.82      8582\n",
      "        love       0.66      0.70      0.68      2063\n",
      "         sad       0.78      0.81      0.80      7234\n",
      "     suprise       0.56      0.65      0.60       923\n",
      "\n",
      "    accuracy                           0.78     25365\n",
      "   macro avg       0.74      0.74      0.74     25365\n",
      "weighted avg       0.78      0.78      0.78     25365\n",
      "\n",
      "F1 Score: 0.7802174019905194\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train Random Forest and evaluate on test set.\n",
    "\n",
    "    Args:\n",
    "        X_train, y_train: Training features and labels.\n",
    "        X_test, y_test: Testing features and labels.\n",
    "\n",
    "    Returns:\n",
    "        model: Trained Random Forest model.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    \"\"\"  param_grid = {\n",
    "    'n_estimators': [100, 200, 300],           # عدد الأشجار\n",
    "    'max_depth': [None, 10, 20, 30],           # أقصى عمق للشجرة\n",
    "    'min_samples_split': [2, 5, 10]             # أقل عدد عينات لتقسيم عقدة\n",
    "    } \"\"\"\n",
    "    \"\"\"  \n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid,cv=3, n_jobs=-1, verbose=2, scoring='f1_weighted')\n",
    "    grid_search.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    print(\"أفضل المعاملات:\", grid_search.best_params_)\n",
    "    print(\"أفضل نتيجة F1 (weighted):\", grid_search.best_score_)\n",
    "    \n",
    "    best_rf_model = grid_search.best_estimator_ \"\"\"\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Test set F1 Score (weighted): {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    \"\"\" cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show() \"\"\"\n",
    "    \n",
    "    print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "    return model\n",
    "\n",
    "random_forest_model = train_and_evaluate(X_train_balanced, y_train_balanced, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from sklearn.model_selection import RandomizedSearchCV\\nfrom scipy.stats import randint\\n\\nparam_dist = {\\n    \\'n_estimators\\': randint(100, 500),\\n    \\'max_depth\\': [None] + list(range(5, 31, 5)),\\n    \\'min_samples_split\\': randint(2, 11)\\n}\\n\\nrf = RandomForestClassifier(random_state=42)\\n\\nrandom_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist,\\n                                   n_iter=20, cv=3, n_jobs=-1, verbose=2, scoring=\\'f1_weighted\\', random_state=42)\\n\\nrandom_search.fit(X_train_balanced, y_train_balanced)\\n\\nprint(\"أفضل المعاملات:\", random_search.best_params_)\\nprint(\"أفضل نتيجة F1 (weighted):\", random_search.best_score_)\\n\\nbest_rf_model = random_search.best_estimator_\\n\\ny_pred = best_rf_model.predict(X_test)\\nprint(f\"Test set F1 Score (weighted): {f1_score(y_test, y_pred, average=\\'weighted\\'):.4f}\")\\nprint(classification_report(y_test, y_pred))\\n '"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'max_depth': [None] + list(range(5, 31, 5)),\n",
    "    'min_samples_split': randint(2, 11)\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist,\n",
    "                                   n_iter=20, cv=3, n_jobs=-1, verbose=2, scoring='f1_weighted', random_state=42)\n",
    "\n",
    "random_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"أفضل المعاملات:\", random_search.best_params_)\n",
    "print(\"أفضل نتيجة F1 (weighted):\", random_search.best_score_)\n",
    "\n",
    "best_rf_model = random_search.best_estimator_\n",
    "\n",
    "y_pred = best_rf_model.predict(X_test)\n",
    "print(f\"Test set F1 Score (weighted): {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' importances = random_forest_model.feature_importances_\\nindices = np.argsort(importances)[-20:]  # أهم 20\\nplt.figure(figsize=(10,6))\\nplt.barh(range(len(indices)), importances[indices])\\nplt.title(\"Top 20 Feature Importances\")\\nplt.show()\\n '"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" importances = random_forest_model.feature_importances_\n",
    "indices = np.argsort(importances)[-20:]  # أهم 20\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(range(len(indices)), importances[indices])\n",
    "plt.title(\"Top 20 Feature Importances\")\n",
    "plt.show()\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' models = {\\n    \\'Random Forest\\': RandomForestClassifier(),\\n    \\'Naive Bayes\\': MultinomialNB(),\\n    \\'SVM\\': SVC(),\\n    \\'KNN\\': KNeighborsClassifier()\\n}\\n\\nfor name, model in models.items():\\n    model.fit(X_train_balanced, y_train_balanced)\\n    y_pred = model.predict(X_test)\\n    print(f\"{name} Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\\n    print(f\"{name} F1 Score: {f1_score(y_test, y_pred, average=\\'weighted\\'):.2f}\")\\n    print(\"-\" * 30)\\n '"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" models = {\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'SVM': SVC(),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"{name} Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "    print(f\"{name} F1 Score: {f1_score(y_test, y_pred, average='weighted'):.2f}\")\n",
    "    print(\"-\" * 30)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy\n"
     ]
    }
   ],
   "source": [
    "def predict_emotion(text, model, vectorizer, label_encoder, scaler):\n",
    "    processed = preprocess_text(text)\n",
    "    \n",
    "    # تحويل النص إلى TF-IDF\n",
    "    vect_text = vectorizer.transform([processed])\n",
    "    \n",
    "    # حساب الميزات الإضافية\n",
    "    additional_feats = np.array([extract_additional_features(processed)])\n",
    "    additional_feats_scaled = scaler.transform(additional_feats)\n",
    "    \n",
    "    # دمج الميزات\n",
    "    from scipy.sparse import hstack, csr_matrix\n",
    "    combined_features = hstack([vect_text, csr_matrix(additional_feats_scaled)])\n",
    "    \n",
    "    # التنبؤ\n",
    "    prediction = model.predict(combined_features)\n",
    "    return label_encoder.inverse_transform(prediction)[0]\n",
    "\n",
    "\n",
    "# مثال تجربة\n",
    "print(predict_emotion(\"I feel so happy and joyful today!\", random_forest_model, tfidf, le, scaler))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: Counter({2: 34326, 4: 28936, 0: 14272, 1: 11980, 3: 8252, 5: 3693})\n",
      "After SMOTE: Counter({2: 34326, 1: 34326, 0: 34326, 4: 34326, 5: 34326, 3: 34326})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"Before SMOTE:\", Counter(y_train))\n",
    "print(\"After SMOTE:\", Counter(y_train_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2767052</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>Suzanne Collins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>J.K. Rowling, Mary GrandPré</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41865</th>\n",
       "      <td>Twilight</td>\n",
       "      <td>Stephenie Meyer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>Harper Lee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4671</th>\n",
       "      <td>The Great Gatsby</td>\n",
       "      <td>F. Scott Fitzgerald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7130616</th>\n",
       "      <td>Bayou Moon</td>\n",
       "      <td>Ilona Andrews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208324</th>\n",
       "      <td>Means of Ascent</td>\n",
       "      <td>Robert A. Caro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77431</th>\n",
       "      <td>The Mauritius Command</td>\n",
       "      <td>Patrick O'Brian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8565083</th>\n",
       "      <td>Cinderella Ate My Daughter: Dispatches from th...</td>\n",
       "      <td>Peggy Orenstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8914</th>\n",
       "      <td>The First World War</td>\n",
       "      <td>John Keegan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     title  \\\n",
       "book_id                                                      \n",
       "2767052                                   The Hunger Games   \n",
       "3                 Harry Potter and the Philosopher's Stone   \n",
       "41865                                             Twilight   \n",
       "2657                                 To Kill a Mockingbird   \n",
       "4671                                      The Great Gatsby   \n",
       "...                                                    ...   \n",
       "7130616                                         Bayou Moon   \n",
       "208324                                    Means of Ascent    \n",
       "77431                                The Mauritius Command   \n",
       "8565083  Cinderella Ate My Daughter: Dispatches from th...   \n",
       "8914                                   The First World War   \n",
       "\n",
       "                             authors  \n",
       "book_id                               \n",
       "2767052              Suzanne Collins  \n",
       "3        J.K. Rowling, Mary GrandPré  \n",
       "41865                Stephenie Meyer  \n",
       "2657                      Harper Lee  \n",
       "4671             F. Scott Fitzgerald  \n",
       "...                              ...  \n",
       "7130616                Ilona Andrews  \n",
       "208324                Robert A. Caro  \n",
       "77431                Patrick O'Brian  \n",
       "8565083              Peggy Orenstein  \n",
       "8914                     John Keegan  \n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# تحميل قاعدة بيانات التوصية\n",
    "books_db = pd.read_csv(\"books.csv\", index_col='book_id')\n",
    "books_db = books_db[['original_title', 'authors']]\n",
    "books_db.rename(columns={'original_title': 'title'}, inplace=True)\n",
    "books_db\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10000 entries, 2767052 to 8914\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    9415 non-null   object\n",
      " 1   authors  10000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 234.4+ KB\n"
     ]
    }
   ],
   "source": [
    "books_db.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_db.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://towardsdatascience.com/not-all-rainbow...</td>\n",
       "      <td>Not All Rainbows and Sunshine: The Darker Side...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://towardsdatascience.com/ethics-in-ai-po...</td>\n",
       "      <td>Ethics in AI: Potential Root Causes for Biased...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://towardsdatascience.com/python-tuple-th...</td>\n",
       "      <td>Python Tuple, The Whole Truth and Only the Tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://towardsdatascience.com/dates-and-subqu...</td>\n",
       "      <td>Dates and Subqueries in SQL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://towardsdatascience.com/temporal-differ...</td>\n",
       "      <td>Temporal Differences with Python: First Sample...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494</th>\n",
       "      <td>https://medium.com/swlh/brian-chesky-is-an-exa...</td>\n",
       "      <td>Brian Chesky is an Example of What it Means to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>https://medium.com/swlh/5-red-flags-of-online-...</td>\n",
       "      <td>5 Red Flags of Online Business Gurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>https://writingcooperative.com/recognizing-the...</td>\n",
       "      <td>Recognizing These Three Realities Can Help Set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>https://writingcooperative.com/i-remember-it-l...</td>\n",
       "      <td>“I Remember It Like It Was Just Yesterday…” Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>https://writingcooperative.com/how-to-formulat...</td>\n",
       "      <td>How to Formulate a Great Nonfiction Theme</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2498 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    url  \\\n",
       "id                                                        \n",
       "1     https://towardsdatascience.com/not-all-rainbow...   \n",
       "2     https://towardsdatascience.com/ethics-in-ai-po...   \n",
       "3     https://towardsdatascience.com/python-tuple-th...   \n",
       "4     https://towardsdatascience.com/dates-and-subqu...   \n",
       "5     https://towardsdatascience.com/temporal-differ...   \n",
       "...                                                 ...   \n",
       "2494  https://medium.com/swlh/brian-chesky-is-an-exa...   \n",
       "2495  https://medium.com/swlh/5-red-flags-of-online-...   \n",
       "2496  https://writingcooperative.com/recognizing-the...   \n",
       "2497  https://writingcooperative.com/i-remember-it-l...   \n",
       "2498  https://writingcooperative.com/how-to-formulat...   \n",
       "\n",
       "                                                  title  \n",
       "id                                                       \n",
       "1     Not All Rainbows and Sunshine: The Darker Side...  \n",
       "2     Ethics in AI: Potential Root Causes for Biased...  \n",
       "3     Python Tuple, The Whole Truth and Only the Tru...  \n",
       "4                           Dates and Subqueries in SQL  \n",
       "5     Temporal Differences with Python: First Sample...  \n",
       "...                                                 ...  \n",
       "2494  Brian Chesky is an Example of What it Means to...  \n",
       "2495               5 Red Flags of Online Business Gurus  \n",
       "2496  Recognizing These Three Realities Can Help Set...  \n",
       "2497  “I Remember It Like It Was Just Yesterday…” Re...  \n",
       "2498          How to Formulate a Great Nonfiction Theme  \n",
       "\n",
       "[2498 rows x 2 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# تحميل داتاست المقالات\n",
    "articles_db = pd.read_csv(\"articles.csv\", index_col='id')  # يحتوي على title, link\n",
    "articles_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2498 entries, 1 to 2498\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   url     2498 non-null   object\n",
      " 1   title   2498 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 58.5+ KB\n"
     ]
    }
   ],
   "source": [
    "articles_db.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# تحميل النموذج و TF-IDF\n",
    "model = joblib.load(\"./models/random_forest_model.pkl\")\n",
    "vectorizer = joblib.load(\"./models/tfidf_vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack, csr_matrix\n",
    "from typing import List\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "def classify_emotions(\n",
    "    df: pd.DataFrame,\n",
    "    text_column: str,\n",
    "    model: BaseEstimator,\n",
    "    vectorizer: TfidfVectorizer,\n",
    "    scaler: StandardScaler,\n",
    "    le: LabelEncoder,\n",
    "    positive_emotions: List[str] = [\"joy\", \"love\", \"surprise\"],\n",
    "    save_path: str = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    يصنف المشاعر في عمود نصي ويُرجع فقط الصفوف ذات المشاعر الإيجابية.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): داتا فريم يحتوي على النصوص.\n",
    "        text_column (str): اسم العمود الذي يحتوي على النصوص (مثل: \"title\" أو \"book_title\").\n",
    "        model (BaseEstimator): نموذج تصنيف مدرب.\n",
    "        vectorizer (TfidfVectorizer): محول TF-IDF.\n",
    "        scaler (StandardScaler): مقياس الميزات الإضافية.\n",
    "        le (LabelEncoder): مشفر العواطف.\n",
    "        positive_emotions (List[str], optional): قائمة بالعواطف الإيجابية للاحتفاظ بها.\n",
    "        save_path (str, optional): إذا تم توفيره، سيتم حفظ الداتا فريم النهائي في هذا المسار كملف CSV.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: داتا فريم يحتوي على النصوص المصنفة ذات العواطف الإيجابية.\n",
    "    \"\"\"\n",
    "\n",
    "    # تحقق من وجود العمود\n",
    "    if text_column not in df.columns:\n",
    "        raise ValueError(f\"❌ العمود '{text_column}' غير موجود في الداتا فريم.\")\n",
    "\n",
    "    # تنظيف النصوص\n",
    "    df['clean_text'] = df[text_column].apply(preprocess_text)\n",
    "\n",
    "    # استخراج ميزات TF-IDF\n",
    "    X_text = vectorizer.transform(df['clean_text'])\n",
    "\n",
    "    # استخراج الميزات الإضافية\n",
    "    additional_features = np.array([\n",
    "        extract_additional_features(text) for text in df['clean_text']\n",
    "    ])\n",
    "    additional_scaled = scaler.transform(additional_features)\n",
    "\n",
    "    # دمج الميزات\n",
    "    combined_features = hstack([X_text, csr_matrix(additional_scaled)])\n",
    "\n",
    "    # التنبؤ بالعاطفة\n",
    "    predicted = model.predict(combined_features)\n",
    "    df['emotion'] = le.inverse_transform(predicted)\n",
    "\n",
    "    # الاحتفاظ فقط بالعواطف الإيجابية\n",
    "    df_filtered = df[df['emotion'].isin(positive_emotions)]\n",
    "\n",
    "\n",
    "    # حفظ إذا تم تحديد مسار\n",
    "    if save_path:\n",
    "        df_filtered.to_csv(save_path, index=False)\n",
    "\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_clean = classify_emotions(\n",
    "        df=books_db,\n",
    "        text_column=\"title\",\n",
    "        model=model,\n",
    "        vectorizer=vectorizer,\n",
    "        scaler=scaler,\n",
    "        le=le,\n",
    "        save_path=\"classified_books.csv\"\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_clean = classify_emotions(\n",
    "        df=articles_db,\n",
    "        text_column=\"title\",\n",
    "        model=model,\n",
    "        vectorizer=vectorizer,\n",
    "        scaler=scaler,\n",
    "        le=le,\n",
    "        save_path=\"classified_articles.csv\"\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_keywords(df: pd.DataFrame, include_keywords=None, exclude_keywords=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ترشيح المحتوى بناءً على كلمات مفتاحية.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): جدول المحتوى.\n",
    "        include_keywords (list): كلمات يجب أن يحتويها النص.\n",
    "        exclude_keywords (list): كلمات يجب ألا يحتويها النص.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: محتوى بعد التصفية.\n",
    "    \"\"\"\n",
    "    if include_keywords:\n",
    "        pattern = '|'.join(include_keywords)\n",
    "        df = df[df['title'].str.contains(pattern, case=False, na=False)]\n",
    "    \n",
    "    if exclude_keywords:\n",
    "        pattern = '|'.join(exclude_keywords)\n",
    "        df = df[~df['title'].str.contains(pattern, case=False, na=False)]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "mood_map = {\n",
    "    'sadness': ['joy', 'surprise'],\n",
    "    'anger': ['love', 'joy'],\n",
    "    'fear': ['love', 'joy'],\n",
    "    'joy': ['joy','surprise', 'love'],\n",
    "    'surprise': ['love', 'joy'],\n",
    "    'love': ['joy', 'love', 'surprise']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    # تنظيف أسماء الأعمدة من المسافات الزائدة\\n    rec_db.columns = rec_db.columns.str.strip()\\n\\n    # التأكد من وجود الأعمدة المطلوبة\\n    if \\'predicted_emotion\\' not in rec_db.columns or \\'original_title\\' not in rec_db.columns:\\n        raise KeyError(\"⚠️ تأكد أن الداتا تحتوي على الأعمدة: \\'original_title\\' و \\'predicted_emotion\\'.\")\\n\\n    # التنبؤ بالمشاعر للنص\\n    user_emotion = predict_emotion(user_text, model, vectorizer, label_encoder)\\n    print(f\"🔍 Detected Emotion: {user_emotion}\")\\n\\n    # تحديد المشاعر المستهدفة بناءً على mood_map\\n    target_emotions = mood_map.get(user_emotion, [\\'joy\\', \\'calm\\', \\'confidence\\'])\\n\\n    # ترشيح المحتوى بناءً على المشاعر\\n    recommended = rec_db[rec_db[\\'predicted_emotion\\'].isin(target_emotions)]\\n\\n    # فلترة المحتوى بناءً على الكلمات\\n    filtered = filter_by_keywords(recommended, include_keywords, exclude_keywords)\\n\\n    # في حالة عدم وجود نتائج بعد الفلترة\\n    if filtered.empty:\\n        print(\"⚠️ لا يوجد محتوى يلبي معايير الفلترة. عرض نتائج بدون فلترة.\")\\n        return recommended[[\\'original_title\\', \\'authors\\',\\'predicted_emotion\\']].sample(min(3, len(recommended)))\\n\\n    return filtered[[\\'original_title\\', \\'authors\\', \\'predicted_emotion\\']].sample(min(3, len(filtered)))\\n\\n '"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "def recommend_content_filtered(\n",
    "    user_text: str,\n",
    "    model,\n",
    "    vectorizer,\n",
    "    label_encoder,\n",
    "    rec_db: pd.DataFrame,\n",
    "    include_keywords=None,\n",
    "    exclude_keywords=None\n",
    ") -> pd.DataFrame:\n",
    "\"\"\"\n",
    "#توصية بمحتوى مع فلترة اختيارية بناءً على الكلمات المفتاحية.\n",
    "\"\"\"\n",
    "    # تنظيف أسماء الأعمدة من المسافات الزائدة\n",
    "    rec_db.columns = rec_db.columns.str.strip()\n",
    "\n",
    "    # التأكد من وجود الأعمدة المطلوبة\n",
    "    if 'predicted_emotion' not in rec_db.columns or 'original_title' not in rec_db.columns:\n",
    "        raise KeyError(\"⚠️ تأكد أن الداتا تحتوي على الأعمدة: 'original_title' و 'predicted_emotion'.\")\n",
    "\n",
    "    # التنبؤ بالمشاعر للنص\n",
    "    user_emotion = predict_emotion(user_text, model, vectorizer, label_encoder)\n",
    "    print(f\"🔍 Detected Emotion: {user_emotion}\")\n",
    "\n",
    "    # تحديد المشاعر المستهدفة بناءً على mood_map\n",
    "    target_emotions = mood_map.get(user_emotion, ['joy', 'calm', 'confidence'])\n",
    "\n",
    "    # ترشيح المحتوى بناءً على المشاعر\n",
    "    recommended = rec_db[rec_db['predicted_emotion'].isin(target_emotions)]\n",
    "\n",
    "    # فلترة المحتوى بناءً على الكلمات\n",
    "    filtered = filter_by_keywords(recommended, include_keywords, exclude_keywords)\n",
    "\n",
    "    # في حالة عدم وجود نتائج بعد الفلترة\n",
    "    if filtered.empty:\n",
    "        print(\"⚠️ لا يوجد محتوى يلبي معايير الفلترة. عرض نتائج بدون فلترة.\")\n",
    "        return recommended[['original_title', 'authors','predicted_emotion']].sample(min(3, len(recommended)))\n",
    "\n",
    "    return filtered[['original_title', 'authors', 'predicted_emotion']].sample(min(3, len(filtered)))\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books(emotion: str, top_n: int = 5) -> List[Dict]:\n",
    "    df = pd.read_csv(\"classified_books.csv\")  # يحتوي على title, link\n",
    "\n",
    "    # تصنيف المقالات إذا لسه ما اتصنفتش\n",
    "    if \"emotion\" not in df.columns:\n",
    "        df[\"emotion\"] = df[\"title\"].apply(predict_emotion)\n",
    "\n",
    "    # حدد المشاعر اللي ممكن تحسن المزاج\n",
    "    target_emotions = mood_map.get(emotion, [\"joy\", \"love\", \"surprise\"])\n",
    "\n",
    "    # رشّح المقالات اللي مشاعرها إيجابية ومناسبة لتحسين المزاج\n",
    "    recommended = df[df[\"emotion\"].isin(target_emotions)].head(top_n)\n",
    "\n",
    "    return recommended[[\"title\", \"authors\", \"emotion\"]].to_dict(orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_articles(emotion: str, top_n: int = 5) -> List[Dict]:\n",
    "    df = pd.read_csv(\"classified_articles.csv\")  # يحتوي على title, link\n",
    "\n",
    "    # تصنيف المقالات إذا لسه ما اتصنفتش\n",
    "    if \"emotion\" not in df.columns:\n",
    "        df[\"emotion\"] = df[\"title\"].apply(predict_emotion)\n",
    "\n",
    "    # حدد المشاعر اللي ممكن تحسن المزاج\n",
    "    target_emotions = mood_map.get(emotion, [\"joy\", \"love\", \"surprise\"])\n",
    "\n",
    "    # رشّح المقالات اللي مشاعرها إيجابية ومناسبة لتحسين المزاج\n",
    "    recommended = df[df[\"emotion\"].isin(target_emotions)].head(top_n)\n",
    "\n",
    "    return recommended[[\"title\", \"url\", \"emotion\"]].to_dict(orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_content(emotion: str, top_n: int = 5) -> Dict[str, List[Dict]]:\n",
    "    books = recommend_books(emotion)[:top_n]\n",
    "    articles = recommend_articles(emotion)[:top_n]\n",
    "    return {\n",
    "        \"books\": books,\n",
    "        \"articles\": articles\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books:\n",
      "- Title: The Great Gatsby, Authors: F. Scott Fitzgerald, Emotion: joy\n",
      "- Title: The Hobbit or There and Back Again, Authors: J.R.R. Tolkien, Emotion: joy\n",
      "- Title: The Catcher in the Rye, Authors: J.D. Salinger, Emotion: joy\n",
      "- Title: Angels & Demons , Authors: Dan Brown, Emotion: joy\n",
      "- Title: The Kite Runner , Authors: Khaled Hosseini, Emotion: joy\n",
      "\n",
      "Articles:\n",
      "- Title: Not All Rainbows and Sunshine: The Darker Side of ChatGPT, Link: None, Emotion: joy\n",
      "- Title: Python Tuple, The Whole Truth and Only the Truth: Let’s Dig Deep, Link: None, Emotion: joy\n",
      "- Title: Temporal Differences with Python: First Sample-Based Reinforcement Learning Algorithm, Link: None, Emotion: joy\n",
      "- Title: 10 Subtle Strategies I Wish I Knew When I Had 23 Email Subscribers and Made $0 Online, Link: None, Emotion: joy\n",
      "- Title: Don’t Become a Full-Time Content Creator If You Have Low-Risk Tolerance, Link: None, Emotion: joy\n"
     ]
    }
   ],
   "source": [
    "results = recommend_content(emotion=\"sad\")\n",
    "\n",
    "print(\"Books:\")\n",
    "for item in results.get('books', []):\n",
    "    print(f\"- Title: {item.get('title')}, Authors: {item.get('authors')}, Emotion: {item.get('emotion')}\")\n",
    "\n",
    "print(\"\\nArticles:\")\n",
    "for item in results.get('articles', []):\n",
    "    print(f\"- Title: {item.get('title')}, Link: {item.get('link')}, Emotion: {item.get('emotion')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'The Great Gatsby', 'authors': 'F. Scott Fitzgerald', 'emotion': 'joy'}\n",
      "{'title': 'The Hobbit or There and Back Again', 'authors': 'J.R.R. Tolkien', 'emotion': 'joy'}\n",
      "{'title': 'The Catcher in the Rye', 'authors': 'J.D. Salinger', 'emotion': 'joy'}\n",
      "{'title': 'Angels & Demons ', 'authors': 'Dan Brown', 'emotion': 'joy'}\n",
      "{'title': 'The Kite Runner ', 'authors': 'Khaled Hosseini', 'emotion': 'joy'}\n"
     ]
    }
   ],
   "source": [
    "results = recommend_books(emotion=\"sad\")\n",
    "for book in results:\n",
    "    print(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Not All Rainbows and Sunshine: The Darker Side of\\xa0ChatGPT', 'url': 'https://towardsdatascience.com/not-all-rainbows-and-sunshine-the-darker-side-of-chatgpt-75917472b9c', 'emotion': 'joy'}\n",
      "{'title': 'Python Tuple, The Whole Truth and Only the Truth: Let’s Dig\\xa0Deep', 'url': 'https://towardsdatascience.com/python-tuple-the-whole-truth-and-only-truth-lets-dig-deep-24d2bf02971b', 'emotion': 'joy'}\n",
      "{'title': 'Temporal Differences with Python: First Sample-Based Reinforcement Learning Algorithm', 'url': 'https://towardsdatascience.com/temporal-differences-with-python-first-sample-based-reinforcement-learning-algorithm-54c11745a0ee', 'emotion': 'joy'}\n",
      "{'title': '10 Subtle Strategies I Wish I Knew When I Had 23 Email Subscribers and Made $0\\xa0Online', 'url': 'https://medium.com/swlh/10-subtle-strategies-i-wish-i-knew-when-i-had-23-email-subscribers-and-made-0-online-3eb65c335060', 'emotion': 'joy'}\n",
      "{'title': 'Don’t Become a Full-Time Content Creator If You Have Low-Risk Tolerance', 'url': 'https://medium.com/swlh/dont-become-a-full-time-content-creator-if-you-have-low-risk-tolerance-13fa2f77791a', 'emotion': 'joy'}\n"
     ]
    }
   ],
   "source": [
    "results = recommend_articles(emotion=\"sad\")\n",
    "for article in results:\n",
    "    print(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/scaler.pkl']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(random_forest_model, './models/random_forest_model.pkl')\n",
    "joblib.dump(tfidf, './models/tfidf_vectorizer.pkl')\n",
    "joblib.dump(le, './models/label_encoder.pkl')\n",
    "joblib.dump(scaler, './models/scaler.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
